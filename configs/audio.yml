define: &f_size 256
define: &low_level_ch 2
define: &t_size 1024
define: &axis_format TFC
define: &virtual_samplerate 48000
define: &HPI False

mapping:
    log_polar: False
    gaussian: False
    gaussian_eps: 0.00001

data:
    dataset: AUDIO
    path: downloads
    dataset_kwargs:
        f_size: *f_size
        t_size: *t_size
        samplerate: *virtual_samplerate
        use_numpy: False
        serve_dtype: torch.float
        device: cuda
        axis: *axis_format
        HPI: *HPI
    
    num_workers: 0

    axis: *axis_format

model:
    dtype: torch.cuda.FloatTensor
    type: simple
    heads: 8
    
    use_attention: [False, False, False, False, True, True]
    res: [2, 2, 2, 4, 2, 4]
    ch: [64, 96, 128, 192, 256, 512]
    
    io:
        patch_size: 1
        channels: *low_level_ch
        t_size: *t_size
        f_size: *f_size

    embedding:
        gamma: 4
        scale: 500
        pos_emb_dim: 128

diffusion:
    training_beta: "np.linspace(1e-4, 2e-2, 1000)"
    sampling_beta: "np.linspace(1e-4, 3.5e-1, 50)"

training:
    batch_size: 48
    n_epochs: null
    n_iters: 5000000
    snapshot_freq: 5000
    validation_freq: 2000

sampling:
    batch_size: 64
    last_only: True
    denoise: True
    denoise_device: cuda
    HPI: *HPI
    virtual_samplerate: *virtual_samplerate

    num_samples: 2
    t_size: 8192

optimization:
    optimizer:
        transformer:
            top_level_name:
                - transformer
            weight_decay: 0.00001
            optimizer: "AdamW"
            warmup: 10000
            lr: 0.0005
            min_rate: 0.2
            beta: [0.9, 0.999]
            amsgrad: false
            eps: 0.00000001
        default:
            top_level_name: []
            weight_decay: 0.00001
            optimizer: "AdamW"
            warmup: 10000
            lr: 0.0005
            min_rate: 0.2
            beta: [0.9, 0.999]
            amsgrad: false
            eps: 0.00000001
    
    grad_norm:
        transformer:
            top_level_name: []
            grad_clip: 1
        default:
            top_level_name: []
            grad_clip: 1