define: &f_size 256
define: &low_level_ch 2
define: &flatten_ch 512
define: &t_size 1024
define: &axis_format CTF
define: &virtual_samplerate 48000
define: &HPI False


data:
    dataset: AUDIO
    path: downloads
    dataset_kwargs:
        f_size: *f_size
        t_size: *t_size
        virtual_samplerate: *virtual_samplerate
        use_numpy: False
        dtype: torch.float
        device: cuda
        axis: *axis_format
        HPI: *HPI
    pfft_format: *axis_format
    num_workers: 0

model:
    dtype: torch.cuda.FloatTensor
    type: simple

    transformers:
        imports: "import transformers; from transformers.models.fnet.modeling_fnet import FNetEncoder"
        module: FNetEncoder
        config: transformers.FNetConfig
        kwargs:
            hidden_size: 512
            num_hidden_layers: 12
            intermediate_size: 2048
            hidden_act: gelu_new
            hidden_dropout_prob: 0.1
            initializer_range: 0.02
            layer_norm_eps: 0.000001
        channels: 512
        dtype: torch.cuda.FloatTensor

    channels: *low_level_ch
    t_size: *t_size
    f_size: *f_size

    ch: [32, 64, 96, 128, 192, 256]
    krn: [3, 3, 3, 3, 3, 3]
    res: [2, 2, 3, 3, 3, 3]
    
    var_type: fixedlarge
    ema_rate: 0.9999
    ema: True

diffusion:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 1000

training:
    batch_size: 14
    n_epochs: null
    n_iters: 5000000
    snapshot_freq: 5000
    validation_freq: 2000

sampling:
    batch_size: 64
    last_only: True
    denoise: True
    HPI: *HPI
    virtual_samplerate: *virtual_samplerate

    num_samples: 2
    t_size: 8192

optimization:
    optimizer:
        transformer:
            top_level_name:
                - transformer
            weight_decay: 0.0001
            optimizer: "AdamW"
            warmup: 10000
            lr: 0.0005
            beta: [0.9, 0.998]
            amsgrad: false
            eps: 0.000001
        default:
            top_level_name: []
            weight_decay: 0.00001
            optimizer: "AdaBelief"
            warmup: 1000
            lr: 0.0003
            beta: [0.9, 0.999]
            amsgrad: false
            eps: 0.00000001
            clip_step: null
            norm_ord: 2
    
    grad_norm:
        transformer:
            top_level_name: []
            grad_clip: 1
        default:
            top_level_name: []
            grad_clip: 1